============================================
README.md
Ingestion Layer Documentation
=============================

Lovdata RAG System â€“ Ingestion Layer
Production-ready ingestion pipeline for Norwegian legal documents.

The ingestion layer is responsible for **collecting, preprocessing, chunking, embedding, and storing legal documents** into the vector database used by the RAG backend.

============================================
ğŸ¯ Features
===========

âœ… Lovdata legal document ingestion
âœ… Robust preprocessing & text cleaning
âœ… Advanced chunking (parentâ€“child, token-aware)
âœ… Embedding generation (BGE / OpenAI / Azure OpenAI)
âœ… Milvus vector database storage
âœ… Idempotent & repeatable ingestion runs
âœ… Structured metadata for traceability
âœ… Logging & error handling
âœ… Production-ready modular architecture

============================================
ğŸ“ Project Structure
====================
# Commit
git commit -m "Initial commit - clean codebase with tests"

# Add remote (this will fail if already exists, that's OK)
git remote add origin https://github.com/Vineesh02/Digirett_AI.git

# Force push to overwrite old history
git push -u origin main --force
```

---

### **STEP 11: Verify on GitHub**

1. Go to: https://github.com/Vineesh02/Digirett_AI
2. Refresh the page
3. You should see:
   - âœ… `ingestion/` folder with all your code
   - âœ… `tests/` folder with demo code
   - âœ… `data/` folder (but empty except `.gitkeep`)
   - âœ… Repository size: **< 5 MB**

---
ğŸ“ Project Structure
```
DIGIRETT-AI-AGENT/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ .gitkeep
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ ingestion/
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ processors/
â”‚       â”‚   â”œâ”€â”€ chunker.py
â”‚       â”‚   â”œâ”€â”€ embedder_sagemaker.py
â”‚       â”‚   â””â”€â”€ text_processor.py
â”‚       â”œâ”€â”€ storage/
â”‚       â”‚   â”œâ”€â”€ milvus_store.py
â”‚       â”‚   â””â”€â”€ supabase_store.py
â”‚       â”œâ”€â”€ verify/
â”‚       â”‚   â”œâ”€â”€ check_chunker.py
â”‚       â”‚   â”œâ”€â”€ del_milvus.py
â”‚       â”‚   â”œâ”€â”€ verify_milvus.py
â”‚       â”‚   â””â”€â”€ verify_sagemaker.py
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py
â”‚       â””â”€â”€ main.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ demo_testing.py
â”‚   â”œâ”€â”€ test_bge_embedding.py
â”‚   â”œâ”€â”€ test_collector.py
â”‚   â”œâ”€â”€ test_health.py
â”‚   â”œâ”€â”€ test_milvus_store.py
â”‚   â””â”€â”€ test_supabase_store.py
â”œâ”€â”€ .gitignore
â”œâ”€â”€ ecosystem.config.js
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

============================================
ğŸš€ Quick Start
==============

---

1. Install Dependencies

---

```
cd ingestion
python -m venv .venv
source .venv/bin/activate   # On Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

---

2. Configure Environment

---

Copy `.env.example` to `.env` and update:

```
# Lovdata
LOVDATA_API_KEY=your-lovdata-api-key

# Embeddings
EMBEDDING_PROVIDER=azure_openai
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_API_KEY=your-api-key
AZURE_OPENAI_EMBEDDING_DEPLOYMENT=bge-m3

# Milvus
MILVUS_HOST=localhost
MILVUS_PORT=19530
MILVUS_COLLECTION=lovdata_legal_docs

# Chunking
CHUNK_SIZE=512
CHUNK_OVERLAP=64
```

---

3. Run the Ingestion Pipeline

---

### Standard Ingestion Run

```
python -m ingestion.src.main
```

This command will:

* Fetch legal documents from Lovdata
* Clean and normalize text
* Apply chunking strategy
* Generate embeddings
* Store vectors and metadata in Milvus

============================================
ğŸ§ª Testing
==========

Run ingestion tests:

```
pytest tests/ demo_testing.
```

============================================
ğŸ§  Ingestion Flow
=================

```
Lovdata API
    â†“
Raw Document Loader
    â†“
Text Cleaning & Normalization
    â†“
Chunking (Parentâ€“Child / Token-Aware)
    â†“
Embedding Generation
    â†“
Milvus Vector Store
```

============================================
ğŸ§© Chunking Strategy
====================

* **Parentâ€“Child Chunking**

  * Parent chunk: legal section or article
  * Child chunks: smaller semantic units used for embeddings

* **Token-Aware Chunking**

  * Prevents exceeding LLM token limits
  * Preserves semantic coherence

* **Dynamic Chunk Sizes**

  * Adjusts based on document structure

============================================
ğŸ“Š Logging & Monitoring
=======================

Logs are stored under `logs/`

```
tail -f logs/ingestion.log
```

Log Levels:

* DEBUG
* INFO
* WARNING
* ERROR

============================================
ğŸ”® Future Enhancements
======================

* Incremental ingestion & versioning
* PDF / DOCX ingestion
* Multilingual embeddings
* Deduplication & change detection
* Ingestion metrics dashboard

============================================
Version: 1.0.0
Last Updated: January 2026
